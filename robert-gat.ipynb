{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-07T03:26:04.538907Z",
     "start_time": "2023-12-07T03:26:02.631495Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaModel\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# class TextGraphDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, texts, edge_indices, labels, tokenizer_name='roberta-base'):\n",
    "#         \"\"\"\n",
    "#         texts: 句子列表\n",
    "#         edge_indices: 关系列表\n",
    "#         labels: 标签列表\n",
    "#         tokenizer_name: 使用的预训练tokenizer名称\n",
    "#         \"\"\"\n",
    "#         self.tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name)\n",
    "#         self.texts = texts\n",
    "#         self.edge_indices = edge_indices\n",
    "#         self.labels = labels\n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         edge_index = self.edge_indices[idx]\n",
    "#         label = self.labels[idx]\n",
    "# \n",
    "#         inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#         input_ids = inputs['input_ids'].squeeze(0)  \n",
    "#         attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "# \n",
    "#         return input_ids, attention_mask, edge_index, label\n",
    "    \n",
    "    \n",
    "class GATConvWithAttention(GATConv):\n",
    "    def forward(self, x, edge_index, edge_attr=None, size=None, return_attention_weights=True):\n",
    "        out, attention_weights = super().forward(x, edge_index, edge_attr, size, return_attention_weights)\n",
    "        return out, attention_weights\n",
    "\n",
    "\n",
    "class RobertaGAT(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_classes):\n",
    "        super(RobertaGAT, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.gat = GATConvWithAttention(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, edge_index):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "        gat_output, attention_weights = self.gat(sentence_embeddings, edge_index)\n",
    "        return F.log_softmax(gat_output, dim=1), attention_weights\n",
    "    \n",
    "\n",
    "# 初始化模型\n",
    "model = RobertaGAT(\"roberta-base\", num_classes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T10:59:13.994958Z",
     "start_time": "2023-12-04T10:59:09.745426Z"
    }
   },
   "id": "de9d2d2ed033e6c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "# 图关系\n",
    "# 训练集(71251,4519)\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "llama = Llama(model_path='./llama-2-7b.Q4_K_M.gguf', embedding=True)\n",
    "\n",
    "def get_edge_index(path, num):\n",
    "        # df = pd.read_csv('data/train.tsv', sep='\\t')  # 训练集(71251,4519)\n",
    "        # df = pd.read_csv('data/test.tsv', sep='\\t')   # 测试集合(15250,965)\n",
    "        # df = pd.read_csv(\"data/validation.csv\")  # 验证集 (16073,1028)\n",
    "    df = pd.read_csv(path)\n",
    "    relationship = []\n",
    "    sentences_embedding = []\n",
    "    abstracts_embedding = []\n",
    "    # sentence_rel = []\n",
    "    abstract_rel = []\n",
    "    abstract = ''\n",
    "    for i in range(0, len(df['label'])):\n",
    "        if df['label'][i] == 'titile':\n",
    "            sentence_embedding = llama.create_embedding(input=df[\"text\"][i])\n",
    "            sentences_embedding.append([i ,sentence_embedding.get('data')[0].get('embedding')])\n",
    "            continue\n",
    "        abstract += df['text'][i]\n",
    "        relationship.append([i, num])\n",
    "        if df['label'][i] == 'conclusions' and df['label'][i+1] == 'background':\n",
    "            abstract_embedding = llama.create_embedding(input=abstract)\n",
    "            abstracts_embedding.append([i, abstract_embedding.get('data')[0].get('embedding')])\n",
    "            abstract = ''\n",
    "            num += 1\n",
    "            continue\n",
    "        relationship.append([i, i+1])\n",
    "    abs_dic = {}\n",
    "    for i, j in itertools.combinations(range(len(abstracts_embedding)), 2):\n",
    "        cos = cosine_similarity([abstracts_embedding[i][1], abstracts_embedding[j][1]])\n",
    "        abs_dic[(i,j)] = cos[0][1]\n",
    "    y = 0\n",
    "    for i in abs_dic.keys():\n",
    "        if abs_dic.get(i) >= 0.8:\n",
    "            abstract_rel.append([i[0], i[1]])\n",
    "            y += 1 \n",
    "    abstract_edge = pd.DataFrame(abstract_rel)\n",
    "    abstract_edge.to_csv(\"abstract.csv\")\n",
    "    edge_index = torch.tensor(relationship)\n",
    "    # sentence_edge_index = torch.tensor(relationship)\n",
    "    abstract_edge_index = torch.tensor(abstract_rel)\n",
    "    return edge_index, abstract_edge_index # sentence_edge_index,"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "337f5c87ce2eea22"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 50785.48 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 50782.62 ms /   509 tokens (   99.77 ms per token,    10.02 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 50800.65 ms\n",
      "llama_tokenize_with_model: too many tokens\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (43,) into shape (0,)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[116], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m dataset \u001B[38;5;241m=\u001B[39m DatasetDict({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m: dataset_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m: dataset_test, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m: dataset_valid})\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# 获取关系\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# train_rel = get_edge_index(path='data/train.csv', num=71251)\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m text_rel \u001B[38;5;241m=\u001B[39m get_edge_index(path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/test.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, num\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15250\u001B[39m)\n",
      "Cell \u001B[0;32mIn[115], line 27\u001B[0m, in \u001B[0;36mget_edge_index\u001B[0;34m(path, num)\u001B[0m\n\u001B[1;32m     25\u001B[0m relationship\u001B[38;5;241m.\u001B[39mappend([i, num])\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m][i] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconclusions\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m][i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackground\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 27\u001B[0m     abstract_embedding \u001B[38;5;241m=\u001B[39m llama\u001B[38;5;241m.\u001B[39mcreate_embedding(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mabstract)\n\u001B[1;32m     28\u001B[0m     abstracts_embedding\u001B[38;5;241m.\u001B[39mappend([i, abstract_embedding\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m'\u001B[39m)])\n\u001B[1;32m     29\u001B[0m     abstract \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/llama_cpp/llama.py:821\u001B[0m, in \u001B[0;36mLlama.create_embedding\u001B[0;34m(self, input, model)\u001B[0m\n\u001B[1;32m    819\u001B[0m tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenize(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    820\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m--> 821\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval(tokens)\n\u001B[1;32m    822\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokens)\n\u001B[1;32m    823\u001B[0m total_tokens \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m n_tokens\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/llama_cpp/llama.py:494\u001B[0m, in \u001B[0;36mLlama.eval\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    492\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama_eval returned \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreturn_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m# Save tokens\u001B[39;00m\n\u001B[0;32m--> 494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_ids[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_tokens : \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_tokens \u001B[38;5;241m+\u001B[39m n_tokens] \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m    495\u001B[0m \u001B[38;5;66;03m# Save logits\u001B[39;00m\n\u001B[1;32m    496\u001B[0m rows \u001B[38;5;241m=\u001B[39m n_tokens \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mlogits_all \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mValueError\u001B[0m: could not broadcast input array from shape (43,) into shape (0,)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 加载数据集\n",
    "dataset_train = load_dataset('csv', data_files='data/train.csv')\n",
    "dataset_test = load_dataset('csv', data_files='data/test.csv')\n",
    "dataset_valid = load_dataset('csv', data_files='data/validation.csv')\n",
    "dataset = DatasetDict({'train': dataset_train, 'test': dataset_test, 'validation': dataset_valid})\n",
    "# 获取关系\n",
    "# train_rel = get_edge_index(path='data/train.csv', num=71251)\n",
    "text_rel = get_edge_index(path='data/test.csv', num=15250)\n",
    "# valid_rel = get_edge_index(path='data/validation.csv', num=16073)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T07:15:33.497439Z",
     "start_time": "2023-12-07T07:13:29.291708Z"
    }
   },
   "id": "14b5c2fea9293287"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/71251 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4057d4ed518847f680274020e3e5ec22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/15250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1d791d648bb4388944527df4db26c9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/16073 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b17e4de1a57147448318f15976e9a77f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "def encode_batch(batch):\n",
    "    return RobertaTokenizer(batch['text'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "dataset = {split: dataset[split].map(encode_batch, batched=True) for split in dataset.keys()}\n",
    "\n",
    "data_split = dataset['train'] \n",
    "\n",
    "input_ids = torch.stack(tuple(data_split['input_ids']))\n",
    "attention_mask = torch.stack(tuple(data_split['attention_mask']))\n",
    "\n",
    "# edge_index = train_rel\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T11:16:14.284617Z",
     "start_time": "2023-12-04T11:15:36.050920Z"
    }
   },
   "id": "b8943632c32c1976"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 假设标签\n",
    "labels = torch.tensor([0, 1], dtype=torch.long)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_ids, attention_mask, edge_index)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T11:22:12.875401Z",
     "start_time": "2023-12-04T11:22:03.545964Z"
    }
   },
   "id": "29ac6c20a23db9d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
