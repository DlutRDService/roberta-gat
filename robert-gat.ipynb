{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-20T05:18:11.657841600Z",
     "start_time": "2023-12-20T05:18:11.651208200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaModel\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from llama_cpp import Llama\n",
    "import csv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class GATConvWithAttention(GATConv):\n",
    "    def forward(self, x, edge_index, edge_attr=None, size=None, return_attention_weights=True):\n",
    "        out, attention_weights = super().forward(x, edge_index, edge_attr, size, return_attention_weights)\n",
    "        return out, attention_weights\n",
    "\n",
    "class RobertaGAT(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_classes):\n",
    "        super(RobertaGAT, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.gat = GATConvWithAttention(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, edge_index):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        print(sentence_embeddings, edge_index, len(sentence_embeddings), len(edge_index))\n",
    "    \n",
    "        gat_output, attention_weights = self.gat(sentence_embeddings, edge_index)\n",
    "        return F.log_softmax(gat_output, dim=1), attention_weights\n",
    "    \n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encoded_dataset, edge_index):\n",
    "        self.encoded_dataset = encoded_dataset\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.encoded_dataset[idx]['label'], dtype=torch.long)\n",
    "        input_ids = torch.tensor(self.encoded_dataset[idx]['input_ids'], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.encoded_dataset[idx]['attention_mask'], dtype=torch.long)\n",
    "\n",
    "        edge_index = self.edge_index[idx]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label,  \n",
    "            'edge_index': edge_index\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T04:49:40.634287900Z",
     "start_time": "2023-12-20T04:49:40.626029700Z"
    }
   },
   "id": "de9d2d2ed033e6c9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llama = Llama(model_path='./llama-2-7b.Q4_K_M.gguf', embedding=True, n_ctx=4096, n_gpu_layers=30)\n",
    "\n",
    "# 图关系\n",
    "# 训练集(71251,4519)\n",
    "# 测试集合(15250,965)\n",
    "# 验证集 (16073,1028)\n",
    "def get_sentence_rel(path, num):\n",
    "    \"\"\"\n",
    "    以文章为单位，构建关系（abs_sentence-title）\n",
    "    :param path: \n",
    "    :param num: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, encoding=\"GB2312\")\n",
    "    relationship = []\n",
    "    for i in range(0, len(df['label'])):\n",
    "        if df['label'][i] == 4 and (df['label'][i + 1] == 0 or df['label'][i + 1] == 5):\n",
    "            relationship.append([i, i+1])\n",
    "            num += 1\n",
    "            continue\n",
    "        if df[\"label\"][i] != 5:\n",
    "            relationship.append([i, num])\n",
    "            relationship.append([i, i+1])\n",
    "\n",
    "    return relationship\n",
    "\n",
    "def get_abstract_embedding(path, start, type):\n",
    "    \"\"\"\n",
    "    Llama编码获取摘要embedding。处理结果为[[][]]\n",
    "    :param type: \n",
    "    :param path: \n",
    "    :param start: \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, encoding='GB2312')\n",
    "    abstract = ''\n",
    "    for i in range(start, len(df['label'])):\n",
    "        try:\n",
    "            abstract += df['text'][i]\n",
    "            if df['label'][i] == 4 and df['label'][i + 1] == 0:\n",
    "                abstract_embedding = llama.create_embedding(input=abstract).get('data')[0].get('embedding')\n",
    "                abstract_embedding = np.array(abstract_embedding)\n",
    "                np.save(f\"./temp/abstract_embedding{i}.npy\", abstract_embedding)\n",
    "                with open(f'./data/abstract_embedding_{type}.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow([abstract, f'abstract_embedding{i}.npy'])\n",
    "                abstract = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    tmp = []   \n",
    "    files = os.listdir(\"./temp\", )\n",
    "    # 获取每个文件的完整路径\n",
    "    full_paths = [os.path.join(\"./temp\", file) for file in files]\n",
    "    # 按创建时间对文件进行排序\n",
    "    sorted_files = sorted(full_paths, key=os.path.getctime)\n",
    "    for file in sorted_files:\n",
    "        if file.endswith('.npy'):\n",
    "            tmp.append(np.load(f'{file}', allow_pickle=True))\n",
    "    np.array(tmp)\n",
    "    np.save(f'./data/abstract_embedding_{type}.npy', tmp)\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return cosine_similarity([a, b])[0][1]\n",
    "\n",
    "def get_paper_rel(array, num):\n",
    "    \"\"\"\n",
    "    获取文章直接的关系（title-title）\n",
    "    :param array: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    rels = []\n",
    "    for i in range(0, len(array)):\n",
    "        for j in range(i+1, len(array)):\n",
    "                cos = cos_sim(array[i], array[j])\n",
    "                if cos >= 0.93:\n",
    "                   rels.append([num + i, num + j])\n",
    "    return rels \n",
    "\n",
    "def get_edge_index(sen_rel, abs_rel):\n",
    "    \"\"\"\n",
    "    按节点，构建图关系\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('data/test.csv')\n",
    "    rels = []\n",
    "    for i in range(len(df['label'])):\n",
    "        rel = []\n",
    "        for j in (sen_rel + abs_rel):\n",
    "            if i in j:\n",
    "                rel.append(j)            \n",
    "        rels.append(rel)\n",
    "    return rels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T05:18:17.546744600Z",
     "start_time": "2023-12-20T05:18:13.862936Z"
    }
   },
   "id": "337f5c87ce2eea22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llama.create_embedding(input=\"hellow world\").get('data')[0].get('embedding')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acb43e8ef746de00"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "get_abstract_embedding(path='data/test.csv', start=0, type='test')\n",
    "# get_abstract_embedding(path='data/validation.csv', start=0, type='valid')\n",
    "# get_abstract_embedding(path='data/train.csv', start=21173, type='train')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T07:53:59.316725700Z",
     "start_time": "2023-12-20T07:22:59.169780Z"
    }
   },
   "id": "84bde42d28abd632"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 加载数据集\n",
    "dataset_train = load_dataset('csv', data_files='data/train.csv', encoding='utf-8')\n",
    "dataset_test = load_dataset('csv', data_files='data/test.csv', encoding='utf=8')\n",
    "dataset_valid = load_dataset('csv', data_files='data/validation.csv', encoding='utf-8')\n",
    "dataset = DatasetDict({'train': dataset_train, 'test': dataset_test, 'validation': dataset_valid})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T05:17:38.778643100Z"
    }
   },
   "id": "14b5c2fea9293287"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取边关系\n",
    "# 训练集合边关系\n",
    "# train_sen_rel = get_sentence_rel(path='data/train.csv', num=71251)\n",
    "# train_abs_rel = np.load('data/abstract_embedding_train.npy')\n",
    "# train_paper_rel = get_paper_rel(train_abs_rel, num=71251)\n",
    "# train_rel = get_edge_index(train_sen_rel, train_paper_rel)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T05:17:38.781267200Z",
     "start_time": "2023-12-20T05:17:38.780243400Z"
    }
   },
   "id": "f26eb199a8905764"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试集合边关系\n",
    "test_sen_rel = get_sentence_rel(path='data/test.csv', num=15250)\n",
    "test_abs_rel = np.load('data/abstract_embedding_test.npy')\n",
    "test_paper_rel = get_paper_rel(test_abs_rel, num=15250)\n",
    "test_rel = get_edge_index(test_sen_rel, test_paper_rel)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T05:17:38.782265Z"
    }
   },
   "id": "fdce53d6e76f9c63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 验证集合边关系\n",
    "# valid_sen_rel = get_sentence_rel(path='data/validation.csv', num=16073)\n",
    "# valid_abs_rel = np.load('data/abstract_embedding_validation.npy')\n",
    "# valid_paper_rel = get_paper_rel(valid_abs_rel, num=16073)\n",
    "# valid_rel = get_edge_index(valid_sen_rel, valid_paper_rel)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T05:17:38.784316900Z"
    }
   },
   "id": "2c4d2835a6a6109a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "def encode_batch(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=2048, return_tensors=\"pt\")\n",
    "\n",
    "dataset = {split: dataset[split].map(encode_batch, batched=True) for split in dataset.keys()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T05:17:38.785353300Z"
    }
   },
   "id": "b8943632c32c1976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    print(input_ids)\n",
    "    print(attention_mask)\n",
    "    print(labels)\n",
    "    # TODO 修改批次处理，在GAT中，batch设置需要节点与关系对应，可以通过完整图的子图。节点的邻居采样、填充和掩码集中策略设置批次。\n",
    "    cum_num_nodes = 0  # 累积节点数\n",
    "    edge_index = torch.tensor([[0,1,3],[1,2,7]])\n",
    "    # for item in batch:\n",
    "        \n",
    "        \n",
    "    #     edge_index_tensor = torch.tensor(item['edge_index'], dtype=torch.long) if isinstance(item['edge_index'], list) else item['edge_index']\n",
    "    # \n",
    "    #     # 调整edge_index的节点索引\n",
    "    #     edge_index_adjusted = edge_index_tensor + cum_num_nodes\n",
    "    #     edge_index.append(edge_index_adjusted.t())  # 转置以确保两行\n",
    "    # \n",
    "    #     # 更新累积节点数\n",
    "    #     num_nodes = item['input_ids'].size(0)  # 假设input_ids的长度等于节点数\n",
    "    #     cum_num_nodes += num_nodes\n",
    "    # \n",
    "    # # 合并edge_index\n",
    "    # edge_index = torch.cat(edge_index, dim=1)\n",
    "    # print(edge_index)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'edge_index': edge_index\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T05:17:38.786386600Z"
    }
   },
   "id": "1978f7ff6407ccb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_data_split = dataset['train'] \n",
    "test_data_split = dataset['test']\n",
    "# valid_data_split = dataset['validation']\n",
    "# train_dataset = CustomDataset(train_data_split['train'], train_rel)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = CustomDataset(test_data_split['train'], test_rel)\n",
    "# print(len(test_rel))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# print(len(test_loader))\n",
    "# valid_dataset = CustomDataset(valid_data_split['train'], train_rel)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T05:17:38.787420900Z"
    }
   },
   "id": "1af521403b8b42d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RobertaGAT(\"roberta-base\", num_classes=5)\n",
    "model = nn.DataParallel(model)\n",
    "model.to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "\n",
    "writer = SummaryWriter('log/robert-gat')\n",
    "\n",
    "def validate(model, valid_loader, criterion, device):\n",
    "    model.eval() \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch in valid_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            edge_index = batch['edge_index'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            output, _ = model(input_ids, attention_mask, edge_index)\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(valid_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T05:17:38.792658700Z",
     "start_time": "2023-12-20T05:17:38.788455900Z"
    }
   },
   "id": "6092069c0e03ef26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        edge_index = batch['edge_index'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, weight = model(input_ids, attention_mask, edge_index)\n",
    "        print(output)\n",
    "        loss = criterion(output, labels)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    writer.add_scalar('Training loss', avg_loss, epoch)\n",
    "    # avg_val_loss, val_accuracy = validate(model, valid_loader, criterion, device)\n",
    "    # writer.add_scalar('Validation Loss', avg_val_loss, epoch)\n",
    "    # writer.add_scalar('Validation Accuracy', val_accuracy, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(test_loader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T05:17:38.789505Z"
    }
   },
   "id": "cb5ba0e5a56a2b99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T05:17:38.790569500Z"
    }
   },
   "id": "ee5b32af7ff99a5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
