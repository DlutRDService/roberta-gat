{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-08T09:32:03.417996Z",
     "start_time": "2023-12-08T09:31:51.361330Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaModel\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GATConvWithAttention(GATConv):\n",
    "    def forward(self, x, edge_index, edge_attr=None, size=None, return_attention_weights=True):\n",
    "        out, attention_weights = super().forward(x, edge_index, edge_attr, size, return_attention_weights)\n",
    "        return out, attention_weights\n",
    "\n",
    "class RobertaGAT(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_classes):\n",
    "        super(RobertaGAT, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.gat = GATConvWithAttention(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, edge_index):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "        gat_output, attention_weights = self.gat(sentence_embeddings, edge_index)\n",
    "        return F.log_softmax(gat_output, dim=1), attention_weights\n",
    "    \n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encoded_dataset, edge_index):\n",
    "        self.encoded_dataset = encoded_dataset\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encoded_dataset.items()}\n",
    "        item['edge_index'] = self.edge_index\n",
    "        return item\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de9d2d2ed033e6c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llama = Llama(model_path='./llama-2-7b.Q4_K_M.gguf', embedding=True, n_ctx=4096)\n",
    "\n",
    "# 图关系\n",
    "# 训练集(71251,4519)\n",
    "# 测试集合(15250,965)\n",
    "# 验证集 (16073,1028)\n",
    "def get_sentence_rel(path, num):\n",
    "    \"\"\"\n",
    "    以文章为单位，构建关系（abs_sentence-title）\n",
    "    :param path: \n",
    "    :param num: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    relationship = []\n",
    "    for i in range(0, len(df['label'])):\n",
    "        if df[\"label\"][i] != 5:\n",
    "            relationship.append([i, num])\n",
    "        if df['label'][i] == 4 and df['label'][i + 1] == 0:\n",
    "            num += 1\n",
    "            continue\n",
    "    return relationship\n",
    "\n",
    "def get_abstract_embedding(path, start):\n",
    "    \"\"\"\n",
    "    Llama编码获取摘要embedding。处理结果为[[][]]\n",
    "    :param path: \n",
    "    :param start: \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    abstract = ''\n",
    "    for i in range(start, len(df['label'])):\n",
    "        abstract += df['text'][i]\n",
    "        if df['label'][i] == 4 and df['label'][i + 1] == 0:\n",
    "            abstract_embedding = llama.create_embedding(input=abstract).get('data')[0].get('embedding')\n",
    "            np.save(f\"./temp/abstract_embedding{i}.npy\", abstract_embedding)\n",
    "            with open('./data/abstract_embedding_test.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([abstract, f'abstract_embedding{i}.npy'])\n",
    "            abstract = ''\n",
    "    tmp = []   \n",
    "    files = os.listdir(\"./temp\", )\n",
    "    # 获取每个文件的完整路径\n",
    "    full_paths = [os.path.join(\"./temp\", file) for file in files]\n",
    "    # 按创建时间对文件进行排序\n",
    "    sorted_files = sorted(full_paths, key=os.path.getctime)\n",
    "    for file in sorted_files:\n",
    "        if file.endswith('.npy'):\n",
    "            tmp.append(np.load(f'{file}', allow_pickle=True))\n",
    "    np.array(tmp)\n",
    "    np.save(f'./data/abstract_embedding_test.npy', tmp)\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return cosine_similarity([a, b])[0][1]\n",
    "\n",
    "def get_paper_rel(array):\n",
    "    \"\"\"\n",
    "    获取文章直接的关系（title-title）\n",
    "    :param array: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    rel = []\n",
    "    for i, j in itertools.combinations(range(len(array)), 2):\n",
    "        cos = cos_sim(array[i], array[j])\n",
    "        if cos >= 0.95:\n",
    "            rel.append([i, j])\n",
    "    return rel  \n",
    "\n",
    "def get_edge_index(sen_rel, abs_rel):\n",
    "    \"\"\"\n",
    "    构建图关系\n",
    "    \"\"\"\n",
    "    return torch.tensor(sen_rel + abs_rel)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "337f5c87ce2eea22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12309.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 55581.14 ms /   704 tokens (   78.95 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 55587.81 ms\n"
     ]
    }
   ],
   "source": [
    "get_abstract_embedding(path='data/test.csv', start=183)\n",
    "# get_abstract_embedding(path='data/validation.csv', start=0)\n",
    "# get_abstract_embedding(path='data/train.csv', start=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84bde42d28abd632"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m dataset \u001B[38;5;241m=\u001B[39m DatasetDict({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m: dataset_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m: dataset_test, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m: dataset_valid})\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# 获取关系\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# train_rel = get_edge_index(path='data/train.csv', num=71251)\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m text_rel \u001B[38;5;241m=\u001B[39m get_edge_index(path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/test.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, num\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15250\u001B[39m)\n",
      "Cell \u001B[0;32mIn[25], line 28\u001B[0m, in \u001B[0;36mget_edge_index\u001B[0;34m(path, num)\u001B[0m\n\u001B[1;32m     26\u001B[0m relationship\u001B[38;5;241m.\u001B[39mappend([i, num])\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m][i] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m][i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 28\u001B[0m     abstract_embedding \u001B[38;5;241m=\u001B[39m llama\u001B[38;5;241m.\u001B[39mcreate_embedding(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mabstract)\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     29\u001B[0m     abstracts_embedding\u001B[38;5;241m.\u001B[39mappend([i, abstract_embedding])\n\u001B[1;32m     30\u001B[0m     np\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./temp/abstract_embedding\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m, abstract_embedding)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/llama_cpp/llama.py:821\u001B[0m, in \u001B[0;36mLlama.create_embedding\u001B[0;34m(self, input, model)\u001B[0m\n\u001B[1;32m    819\u001B[0m tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenize(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    820\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m--> 821\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval(tokens)\n\u001B[1;32m    822\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokens)\n\u001B[1;32m    823\u001B[0m total_tokens \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m n_tokens\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/llama_cpp/llama.py:484\u001B[0m, in \u001B[0;36mLlama.eval\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    482\u001B[0m n_past \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(n_ctx \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_ids))\n\u001B[1;32m    483\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[0;32m--> 484\u001B[0m return_code \u001B[38;5;241m=\u001B[39m llama_cpp\u001B[38;5;241m.\u001B[39mllama_eval(\n\u001B[1;32m    485\u001B[0m     ctx\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx,\n\u001B[1;32m    486\u001B[0m     tokens\u001B[38;5;241m=\u001B[39m(llama_cpp\u001B[38;5;241m.\u001B[39mllama_token \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch))(\u001B[38;5;241m*\u001B[39mbatch),\n\u001B[1;32m    487\u001B[0m     n_tokens\u001B[38;5;241m=\u001B[39mllama_cpp\u001B[38;5;241m.\u001B[39mc_int(n_tokens),\n\u001B[1;32m    488\u001B[0m     n_past\u001B[38;5;241m=\u001B[39mllama_cpp\u001B[38;5;241m.\u001B[39mc_int(n_past),\n\u001B[1;32m    489\u001B[0m     n_threads\u001B[38;5;241m=\u001B[39mllama_cpp\u001B[38;5;241m.\u001B[39mc_int(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_threads),\n\u001B[1;32m    490\u001B[0m )\n\u001B[1;32m    491\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    492\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama_eval returned \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreturn_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/llama_cpp/llama_cpp.py:788\u001B[0m, in \u001B[0;36mllama_eval\u001B[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mllama_eval\u001B[39m(\n\u001B[1;32m    782\u001B[0m     ctx: llama_context_p,\n\u001B[1;32m    783\u001B[0m     tokens,  \u001B[38;5;66;03m# type: Array[llama_token]\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    786\u001B[0m     n_threads: c_int,\n\u001B[1;32m    787\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m--> 788\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _lib\u001B[38;5;241m.\u001B[39mllama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 加载数据集\n",
    "dataset_train = load_dataset('csv', data_files='data/train.csv')\n",
    "dataset_test = load_dataset('csv', data_files='data/test.csv')\n",
    "dataset_valid = load_dataset('csv', data_files='data/validation.csv')\n",
    "dataset = DatasetDict({'train': dataset_train, 'test': dataset_test, 'validation': dataset_valid})\n",
    "# 获取边关系\n",
    "# 训练集合关系\n",
    "train_sen_rel = get_sentence_rel(path='data/train.csv', num=0)\n",
    "train_abs_rel = get_abstract_embedding(path='data/train.csv', start=0)\n",
    "train_paper_rel = get_paper_rel(train_abs_rel)\n",
    "train_rel = get_edge_index(train_sen_rel, train_paper_rel)\n",
    "# 测试集合关系\n",
    "test_sen_rel = get_sentence_rel(path='data/test.csv', num=0)\n",
    "test_abs_rel = get_abstract_embedding(path='data/test.csv', start=0)\n",
    "test_paper_rel = get_paper_rel(test_abs_rel)\n",
    "test_rel = get_edge_index(test_sen_rel, test_paper_rel)\n",
    "# 验证集合关系\n",
    "valid_sen_rel = get_sentence_rel(path='data/validation.csv', num=0)\n",
    "valid_abs_rel = get_abstract_embedding(path='data/validation.csv', start=0)\n",
    "valid_paper_rel = get_paper_rel(valid_abs_rel)\n",
    "valid_rel = get_edge_index(valid_sen_rel, valid_paper_rel)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14b5c2fea9293287"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "def encode_batch(batch):\n",
    "    return RobertaTokenizer(batch['text'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "dataset = {split: dataset[split].map(encode_batch, batched=True) for split in dataset.keys()}\n",
    "\n",
    "data_split = dataset['train'] \n",
    "\n",
    "train_dataset = CustomDataset(dataset['train'], train_rel)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8943632c32c1976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RobertaGAT(\"roberta-base\", num_classes=5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        edge_index = batch['edge_index'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask, edge_index)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6092069c0e03ef26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
