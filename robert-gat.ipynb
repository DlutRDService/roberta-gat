{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-07T09:08:50.609750Z",
     "start_time": "2023-12-07T09:08:50.582716Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaModel\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# class TextGraphDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, texts, edge_indices, labels, tokenizer_name='roberta-base'):\n",
    "#         \"\"\"\n",
    "#         texts: 句子列表\n",
    "#         edge_indices: 关系列表\n",
    "#         labels: 标签列表\n",
    "#         tokenizer_name: 使用的预训练tokenizer名称\n",
    "#         \"\"\"\n",
    "#         self.tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name)\n",
    "#         self.texts = texts\n",
    "#         self.edge_indices = edge_indices\n",
    "#         self.labels = labels\n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         edge_index = self.edge_indices[idx]\n",
    "#         label = self.labels[idx]\n",
    "# \n",
    "#         inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#         input_ids = inputs['input_ids'].squeeze(0)  \n",
    "#         attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "# \n",
    "#         return input_ids, attention_mask, edge_index, label\n",
    "    \n",
    "    \n",
    "class GATConvWithAttention(GATConv):\n",
    "    def forward(self, x, edge_index, edge_attr=None, size=None, return_attention_weights=True):\n",
    "        out, attention_weights = super().forward(x, edge_index, edge_attr, size, return_attention_weights)\n",
    "        return out, attention_weights\n",
    "\n",
    "\n",
    "class RobertaGAT(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_classes):\n",
    "        super(RobertaGAT, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.gat = GATConvWithAttention(self.roberta.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, edge_index):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "        gat_output, attention_weights = self.gat(sentence_embeddings, edge_index)\n",
    "        return F.log_softmax(gat_output, dim=1), attention_weights\n",
    "    \n",
    "\n",
    "# 初始化模型\n",
    "model = RobertaGAT(\"roberta-base\", num_classes=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de9d2d2ed033e6c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "# 图关系\n",
    "# 训练集(71251,4519)\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "llama = Llama(model_path='./llama-2-7b.Q4_K_M.gguf', embedding=True)\n",
    "\n",
    "def get_edge_index(path, num):\n",
    "        # df = pd.read_csv('data/train.tsv', sep='\\t')  # 训练集(71251,4519)\n",
    "        # df = pd.read_csv('data/test.tsv', sep='\\t')   # 测试集合(15250,965)\n",
    "        # df = pd.read_csv(\"data/validation.csv\")  # 验证集 (16073,1028)\n",
    "    df = pd.read_csv(path)\n",
    "    relationship = []\n",
    "    sentences_embedding = []\n",
    "    abstracts_embedding = []\n",
    "    # sentence_rel = []\n",
    "    abstract_rel = []\n",
    "    abstract = ''\n",
    "    for i in range(0, len(df['label'])):\n",
    "        if df['label'][i] == 'titile':\n",
    "            sentence_embedding = llama.create_embedding(input=df[\"text\"][i])\n",
    "            sentences_embedding.append([i ,sentence_embedding.get('data')[0].get('embedding')])\n",
    "            continue\n",
    "        abstract += df['text'][i]\n",
    "        relationship.append([i, num])\n",
    "        if df['label'][i] == 'conclusions' and df['label'][i+1] == 'background':\n",
    "            abstract_embedding = llama.create_embedding(input=abstract)\n",
    "            abstracts_embedding.append([i, abstract_embedding.get('data')[0].get('embedding')])\n",
    "            abstract = ''\n",
    "            num += 1\n",
    "            continue\n",
    "        relationship.append([i, i+1])\n",
    "    abs_dic = {}\n",
    "    for i, j in itertools.combinations(range(len(abstracts_embedding)), 2):\n",
    "        cos = cosine_similarity([abstracts_embedding[i][1], abstracts_embedding[j][1]])\n",
    "        abs_dic[(i,j)] = cos[0][1]\n",
    "    y = 0\n",
    "    for i in abs_dic.keys():\n",
    "        if abs_dic.get(i) >= 0.8:\n",
    "            abstract_rel.append([i[0], i[1]])\n",
    "            y += 1 \n",
    "    abstract_edge = pd.DataFrame(abstract_rel)\n",
    "    abstract_edge.to_csv(\"abstract.csv\")\n",
    "    edge_index = torch.tensor(relationship)\n",
    "    # sentence_edge_index = torch.tensor(relationship)\n",
    "    abstract_edge_index = torch.tensor(abstract_rel)\n",
    "    return edge_index, abstract_edge_index # sentence_edge_index,"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "337f5c87ce2eea22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 20052.54 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 38668.01 ms /   490 tokens (   78.91 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 38677.88 ms\n",
      "\n",
      "llama_print_timings:        load time = 20052.54 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 47703.40 ms /   591 tokens (   80.72 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 47714.65 ms\n",
      "\n",
      "llama_print_timings:        load time = 20052.54 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 49805.91 ms /   614 tokens (   81.12 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 49814.48 ms\n",
      "\n",
      "llama_print_timings:        load time = 20052.54 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 25010.08 ms /   334 tokens (   74.88 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 25015.47 ms\n",
      "\n",
      "llama_print_timings:        load time = 20052.54 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 50150.56 ms /   646 tokens (   77.63 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 50168.52 ms\n",
      "\n",
      "llama_print_timings:        load time = 20052.54 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 34077.88 ms /   464 tokens (   73.44 ms per token,    13.62 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 34083.91 ms\n",
      "\n",
      "llama_print_timings:        load time = 20052.54 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 55845.54 ms /   747 tokens (   74.76 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 55856.00 ms\n",
      "\n",
      "llama_print_timings:        load time = 20052.54 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 46929.40 ms /   601 tokens (   78.09 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 46937.17 ms\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 加载数据集\n",
    "dataset_train = load_dataset('csv', data_files='data/train.csv')\n",
    "dataset_test = load_dataset('csv', data_files='data/test.csv')\n",
    "dataset_valid = load_dataset('csv', data_files='data/validation.csv')\n",
    "dataset = DatasetDict({'train': dataset_train, 'test': dataset_test, 'validation': dataset_valid})\n",
    "# 获取关系\n",
    "# train_rel = get_edge_index(path='data/train.csv', num=71251)\n",
    "text_rel = get_edge_index(path='data/test.csv', num=15250)\n",
    "# valid_rel = get_edge_index(path='data/validation.csv', num=16073)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14b5c2fea9293287"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "def encode_batch(batch):\n",
    "    return RobertaTokenizer(batch['text'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "dataset = {split: dataset[split].map(encode_batch, batched=True) for split in dataset.keys()}\n",
    "\n",
    "data_split = dataset['train'] \n",
    "\n",
    "input_ids = torch.stack(tuple(data_split['input_ids']))\n",
    "attention_mask = torch.stack(tuple(data_split['attention_mask']))\n",
    "\n",
    "# edge_index = train_rel\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b8943632c32c1976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 假设标签\n",
    "labels = torch.tensor([0, 1], dtype=torch.long)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_ids, attention_mask, edge_index)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29ac6c20a23db9d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
